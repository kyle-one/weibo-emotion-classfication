{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84364937-07dc-48a1-9933-a346c4de0dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import jiagu\n",
    "import jieba\n",
    "import torchtext\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "glove_path=\"/Public/YongkunLiu/2021/weibo-course-design/dataset/sgns.weibo.word\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f11cb218-f64d-4845-9b03-33daacd85ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description='WILDCAT Training')\n",
    "\n",
    "parser.add_argument(\"--GLOVE_PATH\", default=\"/Public/YongkunLiu/2021/weibo-course-design/dataset/sgns.weibo.word\",help=\"display a square of a given number\", type=str)\n",
    "parser.add_argument(\"--NUM_EPOCHS\",default=10, help=\"display a square of a given number\", type=int)\n",
    "parser.add_argument(\"--BATCH_SIZE\",default=256, help=\"display a square of a given number\", type=int)\n",
    "parser.add_argument(\"--IMG_SIZE\",default=512, help=\"display a square of a given number\", type=int)\n",
    "parser.add_argument(\"--LR\",default=0.01, help=\"display a square of a given number\", type=float)\n",
    "parser.add_argument(\"--WORK_DIR\",default='/Public/YongkunLiu/weibo-workdir/', help=\"display a square of a given number\", type=str)\n",
    "parser.add_argument(\"--TRAINVAL_PATH\",default=\"./dataset/weibo_senti_100k_trainval_fenci.csv\", help=\"display a square of a given number\", type=str)\n",
    "parser.add_argument(\"--HIDDEN_SIZE\",default=32, help=\"display a square of a given number\", type=int)\n",
    "parser.add_argument(\"--NUM_LAYERS\",default=2, help=\"display a square of a given number\", type=int)\n",
    "parser.add_argument(\"--BIDIRECTIONAL\",default=\"True\", help=\"display a square of a given number\", type=str)\n",
    "parser.add_argument(\"--RNN_TYPE\",default=\"GRU\", help=\"display a square of a given number\", type=str)\n",
    "\n",
    "#bidirectional\n",
    "#rnn_type\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "ctx=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "localtime = time.strftime('%Y-%m-%d-%H-%M-%S',time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16697fcf-60a2-4502-b706-255f52b1858f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time.struct_time(tm_year=2022, tm_mon=1, tm_mday=9, tm_hour=7, tm_min=50, tm_sec=42, tm_wday=6, tm_yday=9, tm_isdst=0)\n"
     ]
    }
   ],
   "source": [
    "print(time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59576200-4749-42ff-a006-d3dc1e638b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'time': '2022-01-09-01-49-05', 'epoch': 10, 'BATCH_SIZE': 256, 'RNN_TYPE': 'GRU', 'BIDIRECTIONAL': 'True', 'NUM_LAYERS': 2, 'HIDDEN_SIZE': 32}\n"
     ]
    }
   ],
   "source": [
    "def get_logger():\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    formatter = logging.Formatter(fmt=\"%(message)s\")\n",
    "\n",
    "    sHandler = logging.StreamHandler()\n",
    "    sHandler.setFormatter(formatter)\n",
    "    logger.addHandler(sHandler)\n",
    "\n",
    "    if not os.path.exists(args.WORK_DIR+'/log'):\n",
    "        os.makedirs(args.WORK_DIR+'/log')\n",
    "        #LOG_DIR +'/'+ RNN_TYPE+'_'+BIDIRECTIONAL+'_split'+str(SPLIT)+'.log'\n",
    "    \n",
    "    fHandler = logging.FileHandler(\"{}/{}.log\".format(args.WORK_DIR+'/log',localtime), mode='w')\n",
    "    fHandler.setLevel(logging.DEBUG)\n",
    "    fHandler.setFormatter(formatter)\n",
    "    logger.addHandler(fHandler)\n",
    "    return logger\n",
    "logger=get_logger()\n",
    "print_json={\"time\":str(localtime),\"epoch\":args.NUM_EPOCHS,\"BATCH_SIZE\":args.BATCH_SIZE,\n",
    "            \"RNN_TYPE\":args.RNN_TYPE,\"BIDIRECTIONAL\":args.BIDIRECTIONAL,\"NUM_LAYERS\":args.NUM_LAYERS,\n",
    "           \"HIDDEN_SIZE\":args.HIDDEN_SIZE}\n",
    "logger.info(print_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d25f80fa-840e-4c5d-b6f1-b8ece12cf27a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [\"\\u6211\", \"\\u4e5f\", \"\\u6562\", \"\\u9732\", \"\\u55...\n",
       "1         [\"\\u4e0b\\u5348\", \"\\u62ff\", \"\\u5feb\\u9012\", \"\\u...\n",
       "2         [\"\\u6211\", \"\\u89c9\\u5f97\", \"\\u5341\\u5927\", \"\\u...\n",
       "3         [\"\\u56de\\u590d\", \"@\", \"\\u8d39\\u6240\\u5efa\", \"j...\n",
       "4         [\"\\u56de\\u590d\", \"@\", \"?\", \"\\u826f\", \"?\", \":\",...\n",
       "                                ...                        \n",
       "107984    [\"\\u56de\\u590d\", \"@\", \"\\u9ed1\", \"_\", \"\\u9c7c\",...\n",
       "107985    [\"\\u606d\\u795d\", \"\\u5927\\u5bb6\", \"\\u9f99\\u5e74...\n",
       "107986    [\"[\", \"\\u5fae\\u98ce\", \"]\", \"\\u53bb\", \"\\u6625\\u...\n",
       "107987    [\"@\", \"\\u652f\\u82f1\\u73c9\", \" \", \"\\u770b\\u770b...\n",
       "107988    [\"\\u4e0d\\u5149\", \"\\u5077\", \"\\u76f8\\u673a\", \"\\u...\n",
       "Name: review, Length: 107989, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#读取数据集 1表示积极，0表示消极,要不要清洗[ ] // @ 之类的符号？\n",
    "file = pd.read_csv(\"./dataset/weibo_senti_100k_trainval_fenci.csv\")\n",
    "df = pd.DataFrame(file)\n",
    "# df.loc[0,'review']=\"haha\"\n",
    "df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bc64e4c-8575-45ab-8949-2eee73443fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum=0\n",
    "# #提前分词，避免CPU瓶颈，将list保存为json字符串格式\n",
    "# file = pd.read_csv(\"./dataset/weibo_senti_100k_trainval.csv\")\n",
    "# df = pd.DataFrame(file)\n",
    "# for idx,i in enumerate(df['review']):\n",
    "#     text_list=jieba.cut(i, cut_all=False)\n",
    "#     text_list=list(text_list)\n",
    "#     text_list=json.dumps(text_list)\n",
    "    \n",
    "#     df.loc[idx,'review']=text_list\n",
    "#     if idx%5000==0:\n",
    "#         print(idx)\n",
    "\n",
    "# df.to_csv(\"./dataset/weibo_senti_100k_trainval_fenci.csv\")\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f40c2e-2e03-4504-a580-a86834964482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acfe1a32-937d-4ee8-b50f-61830184bcc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['hfd','hfdhn']\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"['hfd','hfdhn']\"\n",
    "t=json.dumps(text)\n",
    "t=json.loads(t)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ec9542c-777e-4fcc-ae36-7d596ac9b543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"\\u6211\", \"\\u4e5f\", \"\\u6562\", \"\\u9732\", \"\\u554a\", \"\\uff01\", \"\\u95ee\\u9898\", \"\\u662f\", \"\\u4eba\\u5bb6\", \"\\u8fd9\\u8138\", \"\\u591a\", \"\\u5c0f\", \"\\u554a\", \"\\uff01\", \"[\", \"\\u6293\\u72c2\", \"]\"]\n",
      "['我', '也', '敢', '露', '啊', '！', '问题', '是', '人家', '这脸', '多', '小', '啊', '！', '[', '抓狂', ']']\n"
     ]
    }
   ],
   "source": [
    "text=df['review'][0]\n",
    "print(text)\n",
    "t=json.loads(text)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21ded165-6505-4947-9435-92f7b42a573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = ['train', 'valid']\n",
    "stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.05, random_state=0)\n",
    "#StratifiedShuffleSplit 数据集划分函数\n",
    "#n_splits是train/test的组数，test_size为测试集的比例，andom_state控制是将样本随机打乱\n",
    "train_split_idx, val_split_idx = next(iter(stratified_split.split(df.review, df.label)))\n",
    "train_df = df.iloc[train_split_idx].reset_index()\n",
    "val_df = df.iloc[val_split_idx].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0664fbf9-36c3-4982-ac7a-9586db66a418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_dataset_df(path):\n",
    "    #读取数据集 1表示积极，0表示消极\n",
    "    file = pd.read_csv(path)\n",
    "    df = pd.DataFrame(file)\n",
    "    dataset_names = ['train', 'valid']\n",
    "    stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.05, random_state=0)\n",
    "    #StratifiedShuffleSplit 数据集划分函数\n",
    "    #n_splits是train/test的组数，test_size为测试集的比例，andom_state控制是将样本随机打乱\n",
    "    train_split_idx, val_split_idx = next(iter(stratified_split.split(df.review, df.label)))\n",
    "    train_df = df.iloc[train_split_idx].reset_index()\n",
    "    val_df = df.iloc[val_split_idx].reset_index()\n",
    "    return train_df,val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "714d9c3c-56c2-4b28-8bf7-74094ec57676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74473</td>\n",
       "      <td>74473</td>\n",
       "      <td>74473</td>\n",
       "      <td>28033</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"\\u521a\\u6f14\", \"\\u5b8c\", \"\\u8bdd\\u5267\", \"[\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60729</td>\n",
       "      <td>60729</td>\n",
       "      <td>60729</td>\n",
       "      <td>79237</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"[\", \"\\u82b1\\u5fc3\", \"]\", \"/\", \"/\", \"@\", \"JiA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46804</td>\n",
       "      <td>46804</td>\n",
       "      <td>46804</td>\n",
       "      <td>106357</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"\\u6e90\\u4ee3\\u7801\", \"\\u662f\", \"\\u76f8\\u58f0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90949</td>\n",
       "      <td>90949</td>\n",
       "      <td>90949</td>\n",
       "      <td>75928</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"\\u4eb2\", \"\\u8fd8\", \"\\u80fd\", \"\\u770b\\u5230\",...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52156</td>\n",
       "      <td>52156</td>\n",
       "      <td>52156</td>\n",
       "      <td>15421</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"?\", \"?\", \"?\", \"?\", \"?\", \"/\", \"/\", \"@\", \"\\u62...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0  Unnamed: 0  Unnamed: 0.1   index  label  \\\n",
       "0    74473       74473         74473   28033      1   \n",
       "1    60729       60729         60729   79237      0   \n",
       "2    46804       46804         46804  106357      0   \n",
       "3    90949       90949         90949   75928      0   \n",
       "4    52156       52156         52156   15421      1   \n",
       "\n",
       "                                              review  \n",
       "0  [\"\\u521a\\u6f14\", \"\\u5b8c\", \"\\u8bdd\\u5267\", \"[\"...  \n",
       "1  [\"[\", \"\\u82b1\\u5fc3\", \"]\", \"/\", \"/\", \"@\", \"JiA...  \n",
       "2  [\"\\u6e90\\u4ee3\\u7801\", \"\\u662f\", \"\\u76f8\\u58f0...  \n",
       "3  [\"\\u4eb2\", \"\\u8fd8\", \"\\u80fd\", \"\\u770b\\u5230\",...  \n",
       "4  [\"?\", \"?\", \"?\", \"?\", \"?\", \"/\", \"/\", \"@\", \"\\u62...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df,val_df=get_train_val_dataset_df(args.TRAINVAL_PATH)\n",
    "#一条微博最长为260字，那么为多少词呢...\n",
    "train_df[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cf1e860-c38e-4589-ad49-d97e8bff6e89",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12527/131963527.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#     img = self.transform(img)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# return img, label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self,labels_df,maxlen,transform=None):\n",
    "        super().__init__()\n",
    "        self.labels_df = labels_df\n",
    "        self.transform = transform\n",
    "        self.maxlen=maxlen\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels_df.shape[0]\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review=self.labels_df.review[idx]\n",
    "        review=text_pipeline(review,self.maxlen)\n",
    "        label=self.labels_df.label[idx]\n",
    "        return review,label\n",
    "        \n",
    "        \n",
    "        # image_name = config.PATH_IMG+'/'+self.labels_df.id[idx]+'.jpg'\n",
    "        # img = Image.open(image_name)\n",
    "        # label = self.labels_df.label_idx[idx]\n",
    "        # if self.transform:\n",
    "        #     img = self.transform(img)\n",
    "        # return img, label\n",
    "train_dataset=Dataset(train_df,100)\n",
    "val_dataset=Dataset(val_df,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988e4542-d897-45b0-8d2a-47707617c989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0050c3-b8bb-4948-8375-501ea997c2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "960ff851-210c-4ef9-b3cf-aafa2e108f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./dataset/glove.json\",'r',encoding='utf-8') as f:\n",
    "    glove_dict=json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ae486f3-13d9-4245-bb95-7381dc737f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#glove中文词向量,维度为300,转为json\n",
    "# with open(glove_path,'r',encoding='utf-8') as f:\n",
    "#     a=f.readlines()\n",
    "# glove_dict={}\n",
    "# for word in a[1:]:\n",
    "#     glove_dict[word.split(' ')[0]]=[float(j) for j in word.split(' \\n')[0].split(' ')[1:]]\n",
    "# with open(\"./dataset/glove.json\",'w') as f:\n",
    "#     json.dump(glove_dict,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d00d42f2-0086-40f0-9f76-b2f9302185f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 66, 3, 4, 5]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list insert\n",
    "t=[1,2,3,4,5]\n",
    "t.insert(2,66)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bc5be2b-155c-41dc-8b67-9f26f9e7a30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((8, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0280c62e-522f-4614-bc68-c7a5069b3b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 8, 3, 4, 5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list insert\n",
    "t=[1,8,3,4,5]\n",
    "t2=np.array(t)\n",
    "# t.remove(2)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "424b894c-19b7-4192-b3b8-4921a2a745cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将句子分词，并且转化为词向量的模块，maxlen为一个句子的长度\n",
    "#要不要提前分好词，算了...嫌麻烦，先用着\n",
    "#有了这个模块，似乎就不需要词向量嵌入层nn.Embedding层了？\n",
    "def text_pipeline(text,maxlen):\n",
    "    # text_list=jieba.cut(text, cut_all=False)\n",
    "    # text_list=list(text_list)\n",
    "    # print(text_list)\n",
    "    #将glove字典中找不到的词语分为若干个字\n",
    "    text_list=json.loads(text)\n",
    "    print(text_list)\n",
    "    while 1:\n",
    "        flag=0\n",
    "        for idx,i in enumerate(text_list):\n",
    "            #分为最小，若最小也在glove中无，则删除\n",
    "            if i not in glove_dict:\n",
    "                # print(i)\n",
    "                text_list.remove(i)\n",
    "                for j in i[::-1]:\n",
    "                    if j in glove_dict:\n",
    "                        text_list.insert(idx,j)\n",
    "                flag=1\n",
    "                break\n",
    "        if flag==0:\n",
    "            break\n",
    "            \n",
    "    d=maxlen-len(text_list)\n",
    "    # print(text_list)\n",
    "    if d<=0:\n",
    "        text_list=text_list[:maxlen]\n",
    "    # else:\n",
    "    #     for i in range(d):\n",
    "    #         text_list.append(\"。\")\n",
    "            \n",
    "    #构建array，将glove词向量拼接成为(100,300)形状\n",
    "    res=np.zeros((maxlen, 300))\n",
    "    # print(text_list)\n",
    "    for idx,i in enumerate(text_list):\n",
    "        res[idx]=np.array(glove_dict[i])\n",
    "    \n",
    "    # text_list=[torch.tensor(glove_dict[i], dtype=torch.float64) for i in text_list]#torch.tensor(glove_dict[i], dtype=torch.float64)\n",
    "    res=torch.tensor(res,dtype=torch.float32)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7dc6cc8-a84a-4ea4-afbd-3a56f0303c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我今天好开心，要答辩了\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6084/918078589.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"我今天好开心，要答辩了\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"我今天好开心，要答辩了\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"我今天好开心，要答辩了\")\n",
    "test_pipeline(\"我今天好开心，要答辩了\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95ddb5dc-a39a-47ac-9c22-1f17c92fd9ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6735/1419894430.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# text_pipeline(\"我来到了北京大学刚   演\",10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# print(text_pipeline(\"我来到了北京大学刚   演\",100).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtext_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "# text_pipeline(\"我来到了北京大学刚   演\",10)\n",
    "# print(text_pipeline(\"我来到了北京大学刚   演\",100).shape)\n",
    "print(train_df['review'][0])\n",
    "text_pipeline(train_df['review'][0],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a344116f-1381-4a32-9bea-533c59addefe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1=[0 for i in range(280)]\n",
    "list1[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0bc0f1e-f365-42f0-8872-7435d86f274b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for idx,i in enumerate(train_dataset):\n",
    "    print(len(i[0]))\n",
    "    break\n",
    "# list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29d8970d-b3e4-46f7-8e4d-4b3dc81aad54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx,i in enumerate(list1):\n",
    "#     print(idx,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "326d13ef-67e4-4903-9b30-77f1b9bd117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #将数据集分为trainval和test，占比分别为0.9和0.1，仅仅运行一次\n",
    "# file = pd.read_csv(\"./dataset/weibo_senti_100k_trainval.csv\")\n",
    "# df = pd.DataFrame(file)\n",
    "# df\n",
    "# dataset_names = ['train', 'valid']\n",
    "# stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=0)\n",
    "# #StratifiedShuffleSplit 数据集划分函数\n",
    "# #n_splits是train/test的组数，test_size为测试集的比例，andom_state控制是将样本随机打乱\n",
    "# train_split_idx, val_split_idx = next(iter(stratified_split.split(df.review, df.label)))\n",
    "# train_df = df.iloc[train_split_idx].reset_index()\n",
    "# val_df = df.iloc[val_split_idx].reset_index()\n",
    "# train_df.to_csv(\"./dataset/weibo_senti_100k_trainval.csv\")\n",
    "# val_df.to_csv(\"./dataset/weibo_senti_100k_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ef38d0a-b2bc-45e2-b370-20ea1c234f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.432 seconds.\n",
      "Loading model cost 0.432 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Mode: 我/ 来到/ 北京/ 清华大学\n"
     ]
    }
   ],
   "source": [
    "#分词\n",
    "\n",
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=False)\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa29708f-386f-475d-a347-0fe211978f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1be60b9-86d7-44a7-bcc5-fc7b66eecb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(glove_dict['住口']))\n",
    "a=torch.tensor(glove_dict['住口'], dtype=torch.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d746d546-3507-4913-a9fa-d4579545f3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train_dataset:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8d54058-251d-4bb9-bd17-241ec78bc2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter =DataLoader(train_dataset,batch_size=args.BATCH_SIZE,shuffle=False,num_workers=1,drop_last=True)\n",
    "val_iter =DataLoader(val_dataset,batch_size=args.BATCH_SIZE,shuffle=False,num_workers=1,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "23869fb5-9740-4afc-af89-b2d87f645675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx,i in enumerate(train_iter):\n",
    "#     if idx%100==0:\n",
    "#         print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f563f91-6ede-413a-9eae-361976c85ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size=300, hidden_size=32, output_size=2,num_layers=2,bidirectional='True',rnn_type='GRU'):#输入，隐藏层单元，输出，隐藏层数，双向\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        if(bidirectional=='True'):\n",
    "            self.bidirectional=True\n",
    "        elif(bidirectional=='False'):\n",
    "            self.bidirectional=False\n",
    "        if(rnn_type=='GRU'):\n",
    "            self.rnn=nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,bidirectional=self.bidirectional)\n",
    "        elif(rnn_type=='LSTM'):\n",
    "            self.rnn=nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,bidirectional=self.bidirectional)\n",
    "        elif(rnn_type=='RNN'):\n",
    "            self.rnn=nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,bidirectional=self.bidirectional)\n",
    "        \n",
    "        if(bidirectional=='True'):\n",
    "            self.out = nn.Linear(4*hidden_size, output_size)\n",
    "        else:\n",
    "            self.out = nn.Linear(1*hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #print(input.size())\n",
    "        #10*64*8\n",
    "        output1,hidden=self.rnn(input,hidden)\n",
    "        #print(\"output1:{}\".format(output1.size()))\n",
    "        #10*64*32\n",
    "        if(self.bidirectional==True):\n",
    "            output2 = torch.cat((output1[0], output1[-1]), -1)\n",
    "        else:\n",
    "            output2 = output1[-1]\n",
    "        #print(\"output2:{}\".format(output2.size()))\n",
    "        #64*64\n",
    "        #print(\"hidden:{}\".format(hidden.size()))\n",
    "        #层数*64*结点数\n",
    "        #2*64*32\n",
    "        output=self.out(output2)\n",
    "        #print(output.size())\n",
    "        #64*4\n",
    "        #output=F.softmax(output,dim=0)\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "673b8d00-21f6-4c09-89d7-c135735aae7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): GRU(300, 32, num_layers=2, bidirectional=True)\n",
      "  (out): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model=RNN(hidden_size=args.HIDDEN_SIZE,num_layers=args.NUM_LAYERS,bidirectional=args.BIDIRECTIONAL,rnn_type=args.RNN_TYPE)\n",
    "model.to(ctx)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ce16163-668d-43f1-af13-3a45a757716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0015696-98df-4ab0-b90d-dca37f13845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_loss(data_iter, net,hidden,device, epoch):\n",
    "    net.eval()\n",
    "    l_sum, n,val_pred_sum= 0.0, 0,0\n",
    "    with torch.no_grad():\n",
    "        for bidx,(X, y) in enumerate(data_iter):\n",
    "            X=X.float()\n",
    "            X=X.permute(1,0,2)#[32, 10, 8]->[10,32,8]\n",
    "            y=y.to(device)\n",
    "            X=X.to(device)\n",
    "            \n",
    "            y_hat,hidden= net(X,hidden)\n",
    "            loss = criterion(y_hat, y)\n",
    "            \n",
    "            l_sum+=loss\n",
    "            #l_sum +=criterion(y_hat, y).item()\n",
    "            n += X.size()[1]\n",
    "            pred=y_hat.max(1, keepdim=True)[1].view(-1)\n",
    "            pred_sum=pred.eq(y.view_as(pred)).sum().item()\n",
    "            val_pred_sum+=pred_sum\n",
    "    print_json={\"epoch\":str(epoch),\"trainval\":\"val\",\"loss\":str(float(l_sum/(bidx+1))),\"acc\":val_pred_sum/n}\n",
    "    logger.info(print_json)\n",
    "    #print('val,epoch:{},pred:{},loss:{}'.format(epoch,val_pred_sum/n,l_sum/n))\n",
    "    # logger.info(\"[epoch {}][{}][end] val_loss={:.5f},val_acc:{:.5f}({}/{})\".format\\\n",
    "    # (epoch,'val',l_sum/(bidx+1),val_pred_sum/n,int(val_pred_sum),n))\n",
    "    return l_sum / (bidx+1),val_pred_sum/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "737411d0-8115-48b0-b1b9-fd7c74d1e43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,train_iter,val_iter,num_epoch,lr_period,lr_decay):\n",
    "    optimizer = torch.optim.Adam(net.parameters(),lr=args.LR)\n",
    "    hidden=None\n",
    "    Max_Acc=0.0\n",
    "    Min_loss=9999999.9\n",
    "    for epoch in range(num_epoch):\n",
    "        net.train()\n",
    "        n,train_l_sum,train_pred_sum=0,0,0\n",
    "        if epoch > 0 and epoch % lr_period == 0:\n",
    "            optimizer.param_groups[0]['lr']=optimizer.param_groups[0]['lr']*lr_decay\n",
    "        for bidx,(X,Y) in enumerate(train_iter):\n",
    "            # print(bidx)\n",
    "            X=X.float()\n",
    "            X=X.permute(1,0,2)\n",
    "            X=X.to(ctx)\n",
    "            Y=Y.to(ctx)\n",
    "            if(hidden is not None):\n",
    "                if isinstance (hidden, tuple): # LSTM, state:(h, c)  \n",
    "                    hidden[0].to(ctx)\n",
    "                    hidden[1].to(ctx)\n",
    "                    hidden = (hidden[0].detach(), hidden[1].detach())\n",
    "                else:   \n",
    "                    hidden.to(ctx)\n",
    "                    hidden = hidden.detach()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            #print(str(bidx)+'---------------')\n",
    "            #print(hidden)\n",
    "            # print(X,hidden)\n",
    "            y_hat,hidden= net(X,hidden)\n",
    "            #print(y_hat.size())\n",
    "            loss = criterion(y_hat, Y)\n",
    "            #print(y_hat)\n",
    "            #print(y)\n",
    "            #print(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #print(loss)\n",
    "            pred=y_hat.max(1, keepdim=True)[1].view(-1)\n",
    "            # print(pred)\n",
    "            pred_sum=pred.eq(Y.view_as(pred)).sum().item()\n",
    "            #print(pred_sum)\n",
    "            train_l_sum+=loss\n",
    "            train_pred_sum+=pred_sum\n",
    "            n+=X.size(1)\n",
    "            print_json={\"epoch\":str(epoch),\"batch\":str(bidx),\"trainval\":\"train\",\"loss\":str(float(loss)),\"acc\":pred_sum/Y.size()[0]}\n",
    "            logger.info(print_json)\n",
    "\n",
    "        if not os.path.exists(args.WORK_DIR+'params'):\n",
    "            # print(args.WORK_DIR+'params')\n",
    "            os.makedirs(args.WORK_DIR+'params')\n",
    "            #{}\n",
    "        print_json={\"epoch\":str(epoch),\"batch\":\"end\",\"trainval\":\"train\",\"loss\":str(float(train_l_sum/(bidx+1))),\"acc\":train_pred_sum/n}\n",
    "        logger.info(print_json)\n",
    "        valid_loss,valid_acc=evaluate_loss(val_iter, net,hidden,ctx,epoch)\n",
    "        if(valid_acc>Max_Acc):\n",
    "            Max_Acc=valid_acc\n",
    "            model_best=net\n",
    "            print_json={\"epoch\":str(epoch),\"save\":\"save\"}\n",
    "            logger.info(print_json)\n",
    "            torch.save(net,args.WORK_DIR+'params/best_{}.pth'.format(localtime))\n",
    "        # logger.info(\"[epoch {}][{}][end] train_loss={:.5f},loss_classfication={:.5f},loss_confidence={:.5f},train_acc={:.5f}({}/{})\".format\\\n",
    "        #             (epoch,'train',train_l_sum/(bidx+1),loss_classfication_sum/(bidx+1),loss_confidence_sum/(bidx+1),train_pred_sum/n,int(train_pred_sum),n))\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "749857c2-4f46-4a23-9f9b-79f4ff9626a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'epoch': '0', 'batch': '0', 'trainval': 'train', 'loss': '0.6897450685501099', 'acc': 0.5234375}\n",
      "{'epoch': '0', 'batch': '1', 'trainval': 'train', 'loss': '0.8382019400596619', 'acc': 0.51171875}\n",
      "{'epoch': '0', 'batch': '2', 'trainval': 'train', 'loss': '0.7035603523254395', 'acc': 0.48828125}\n",
      "{'epoch': '0', 'batch': '3', 'trainval': 'train', 'loss': '0.6877555251121521', 'acc': 0.4921875}\n",
      "{'epoch': '0', 'batch': '4', 'trainval': 'train', 'loss': '0.6792736053466797', 'acc': 0.54296875}\n",
      "{'epoch': '0', 'batch': '5', 'trainval': 'train', 'loss': '0.6725870370864868', 'acc': 0.60546875}\n",
      "{'epoch': '0', 'batch': '6', 'trainval': 'train', 'loss': '0.668232798576355', 'acc': 0.61328125}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6654/4092839740.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_6654/2312576812.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_iter, val_iter, num_epoch, lr_period, lr_decay)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlr_period\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlr_decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbidx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0;31m# print(bidx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model,train_iter,val_iter,args.NUM_EPOCHS,2,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdf1738-d9a0-4b82-918c-5fc3233ad2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden=None\n",
    "# for idx,(X,Y) in enumerate(train_iter):\n",
    "#     print(Y)\n",
    "#     print(X.shape)\n",
    "#     X=X.to(ctx)\n",
    "#     outputs=model(X,hidden)\n",
    "#     print(outputs)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c063ad0e-924e-4f08-95f5-4c4731afe9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将句子分词，并且转化为词向量的模块，maxlen为一个句子的长度\n",
    "#要不要提前分好词，算了...嫌麻烦，先用着\n",
    "#有了这个模块，似乎就不需要词向量嵌入层nn.Embedding层了？\n",
    "def test_pipeline(text,maxlen):\n",
    "    text_list=jieba.cut(text, cut_all=False)\n",
    "    text_list=list(text_list)\n",
    "    print(text_list)\n",
    "    #将glove字典中找不到的词语分为若干个字\n",
    "    # text_list=json.loads(text)\n",
    "    print(text_list)\n",
    "    while 1:\n",
    "        flag=0\n",
    "        for idx,i in enumerate(text_list):\n",
    "            #分为最小，若最小也在glove中无，则删除\n",
    "            if i not in glove_dict:\n",
    "                # print(i)\n",
    "                text_list.remove(i)\n",
    "                for j in i[::-1]:\n",
    "                    if j in glove_dict:\n",
    "                        text_list.insert(idx,j)\n",
    "                flag=1\n",
    "                break\n",
    "        if flag==0:\n",
    "            break\n",
    "            \n",
    "    d=maxlen-len(text_list)\n",
    "    # print(text_list)\n",
    "    if d<=0:\n",
    "        text_list=text_list[:maxlen]\n",
    "    # else:\n",
    "    #     for i in range(d):\n",
    "    #         text_list.append(\"。\")\n",
    "            \n",
    "    #构建array，将glove词向量拼接成为(100,300)形状\n",
    "    res=np.zeros((maxlen, 300))\n",
    "    # print(text_list)\n",
    "    for idx,i in enumerate(text_list):\n",
    "        res[idx]=np.array(glove_dict[i])\n",
    "    \n",
    "    # text_list=[torch.tensor(glove_dict[i], dtype=torch.float64) for i in text_list]#torch.tensor(glove_dict[i], dtype=torch.float64)\n",
    "    res=torch.tensor(res,dtype=torch.float32)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c48b595d-9903-4735-a73d-7a5089817981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我今天好开心，要答辩了\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.458 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '今天', '好开心', '，', '要', '答辩', '了']\n",
      "['我', '今天', '好开心', '，', '要', '答辩', '了']\n",
      "torch.Size([100, 300])\n",
      "['我', '今天', '好开心', '，', '要', '答辩', '了']\n",
      "['我', '今天', '好开心', '，', '要', '答辩', '了']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3808, -0.1406,  0.0367,  ...,  0.2216,  0.2000,  0.1449],\n",
       "        [ 0.3193, -0.1960,  0.1231,  ..., -0.2689, -0.1319,  0.0397],\n",
       "        [ 0.1812, -0.1460,  0.0240,  ..., -0.0916, -0.4927,  0.0160],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"我今天好开心，要答辩了\")\n",
    "print(test_pipeline(\"我今天好开心，要答辩了\",100).shape)\n",
    "test_pipeline(\"我今天好开心，要答辩了\",100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58706817-c689-43b9-b417-43585702f44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['无语', '了']\n",
      "['无语', '了']\n",
      "torch.Size([100, 300])\n",
      "torch.Size([1, 100, 300])\n",
      "torch.Size([100, 1, 300])\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "ctx=torch.device(\"cpu\")\n",
    "content=\"无语了\"\n",
    "content=test_pipeline(content,100)\n",
    "print(content.shape)\n",
    "content=content.unsqueeze(0)\n",
    "print(content.shape)\n",
    "content=content.to(ctx)\n",
    "content=content.permute(1,0,2)\n",
    "print(content.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8881f1e7-bc10-4bd0-9b16-304d98664c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3536, -0.6084]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(\"/Public/YongkunLiu/weibo-workdir/params/best_2022-01-09-05-19-15.pth\")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model=model.to(ctx)\n",
    "pred=model(content,None)[0]\n",
    "print(pred)\n",
    "pred=pred.max(1, keepdim=True)[1].view(-1).cpu().numpy().tolist()\n",
    "pred\n",
    "# print(model(content,None)[1].shape)\n",
    "# print(model(content,None)[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe2c7e6-330c-4c56-8a8e-a48e78f2f5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "4,1,32\n",
    "100,1,300"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
